{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Auto Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Plan\n",
    "1. Split data into gene and cell features\n",
    "2. Train a AE on each\n",
    "3. Use AE to generate latent features for gene and cell features\n",
    "4. Use to train ensemble (NN, LR, XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join, exists\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from pipeline.data import MoaDataset\n",
    "from pipeline.networks import MoaDenseNet, DenseModule, initialize_weights, recalibrate_layer\n",
    "from pipeline.utils import SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "BATCH_SIZE = 1024\n",
    "N_FOLD = 5\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../data/train_features.csv')\n",
    "train_targets = pd.read_csv('../data/train_targets_scored.csv')\n",
    "train_targets_ns = pd.read_csv('../data/train_targets_nonscored.csv')\n",
    "test_features = pd.read_csv('../data/test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in train_features.columns if '-' in c]\n",
    "all_features = np.vstack([train_features[features].values, test_features[features].values])\n",
    "scaler = StandardScaler()\n",
    "features_t = scaler.fit_transform(all_features)\n",
    "scaler_2 = MinMaxScaler()\n",
    "features_t = scaler_2.fit_transform(features_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 0.5, 72: 1})\n",
    "    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    df = df.drop('cp_type', axis=1)\n",
    "    df.loc[:, features] = scaler_2.transform(scaler.transform(df.loc[:, features]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = process_df(train_features)\n",
    "test_features = process_df(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "    def load(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def add(self, x):\n",
    "        return self.data.append(x)\n",
    "\n",
    "    def mean(self):\n",
    "        return np.mean(self.data)\n",
    "\n",
    "    def min_epoch(self):\n",
    "        return np.argmin(self.data)\n",
    "\n",
    "    def min(self):\n",
    "        return np.min(self.data)\n",
    "\n",
    "    def max(self):\n",
    "        return np.max(self.data)\n",
    "\n",
    "    def tail(self):\n",
    "        return self.data[-1]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AutoEncoderDataset(Dataset):\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        self.length = len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.x[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, feature_size, latent_layer_size, init_dropout, dropout, normalization, activation):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        bounds = (feature_size, latent_layer_size)\n",
    "        intermediate_layer_size = int(((max(bounds) - min(bounds)) / 2) + min(bounds))\n",
    "        \n",
    "        self.dense_0 = DenseModule(feature_size, intermediate_layer_size, init_dropout, normalization, activation)\n",
    "        self.dense_1 = DenseModule(intermediate_layer_size, latent_layer_size, dropout, normalization, activation)\n",
    "        self.dense_2 = DenseModule(latent_layer_size, intermediate_layer_size, dropout, normalization, activation)\n",
    "        self.dense_3 = DenseModule(intermediate_layer_size, feature_size, dropout, normalization, activation=None)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.dense_1(self.dense_0(x))\n",
    "        logits = self.dense_3(self.dense_2(latent))\n",
    "        return latent, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0,
     1,
     87,
     131,
     157,
     171,
     178,
     188
    ]
   },
   "outputs": [],
   "source": [
    "class NeuralNetModel:\n",
    "    def __init__(self, model_dict):\n",
    "        self.model_dict = model_dict\n",
    "        self.best_weights_path = join(self.model_dict['model_dir'], 'fold-%d-weights.pth' % self.model_dict['fold'])\n",
    "        self.epoch = 0\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.train_metrics = Metrics()\n",
    "        self.valid_metrics = Metrics()\n",
    "\n",
    "        if model_dict['normalization'] == 'batch':\n",
    "            model_dict['normalization'] = nn.BatchNorm1d\n",
    "        else:\n",
    "            Exception('Normalization not supported.')\n",
    "\n",
    "        if model_dict['activation'] == 'relu':\n",
    "            model_dict['activation'] = nn.ReLU\n",
    "        elif model_dict['activation'] == 'prelu':\n",
    "            model_dict['activation'] = nn.PReLU\n",
    "        else:\n",
    "            Exception('Activation not supported.')\n",
    "        \n",
    "        if model_dict['model'] == 'AutoEncoder':\n",
    "            self.model = AutoEncoder(\n",
    "                model_dict['feature_size'],\n",
    "                model_dict['latent_layer_size'],\n",
    "                model_dict['init_dropout'],\n",
    "                model_dict['dropout'],\n",
    "                model_dict['normalization'],\n",
    "                model_dict['activation'],\n",
    "            )\n",
    "        elif model_dict['model'] == 'MoaDenseNet':\n",
    "            self.model = MoaDenseNet(\n",
    "                model_dict['input_dim'],\n",
    "                model_dict['output_dim'],\n",
    "                model_dict['n_hidden_layer'],\n",
    "                model_dict['hidden_dim'],\n",
    "                model_dict['dropout'],\n",
    "                model_dict['activation'],\n",
    "                model_dict['normalization'],\n",
    "            )\n",
    "\n",
    "        # Setup optimizer\n",
    "        if model_dict['optimizer'] == 'sgd':\n",
    "            self.optimizer = optim.SGD(self.model.parameters(),\n",
    "                                       lr=model_dict['learning_rate'],\n",
    "                                       momentum=model_dict['momentum'])\n",
    "        elif model_dict['optimizer'] == 'adam':\n",
    "            self.optimizer = optim.Adam(self.model.parameters(),\n",
    "                                        lr=model_dict['learning_rate'],\n",
    "                                        weight_decay=model_dict['weight_decay'])\n",
    "        else:\n",
    "            Exception('Optimizer not supported.')\n",
    "\n",
    "        # Setup scheduler\n",
    "        if model_dict['scheduler'] == 'ReduceLROnPlateau':\n",
    "            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer,\n",
    "                                                                  patience=3,\n",
    "                                                                  threshold=0.00001)\n",
    "        elif model_dict['scheduler'] == 'OneCycleLR':\n",
    "            self.scheduler = optim.lr_scheduler.OneCycleLR(self.optimizer,\n",
    "                                                           max_lr=model_dict['max_lr'],\n",
    "                                                           pct_start=model_dict['pct_start'],\n",
    "                                                           div_factor=model_dict['div_factor'],\n",
    "                                                           epochs=model_dict['n_epochs'],\n",
    "                                                           steps_per_epoch=model_config['steps_per_epoch'])\n",
    "        else:\n",
    "            Exception('Scheduler not supported.')\n",
    "\n",
    "        # Save initial states of model, optimizer and scheduler\n",
    "        self.init_states = dict(\n",
    "            model=self.model.state_dict(),\n",
    "            optimizer=self.optimizer.state_dict(),\n",
    "            scheduler=self.scheduler.state_dict()\n",
    "        )\n",
    "        self.model = self.model.cuda()\n",
    "\n",
    "    def unwrap_batch(self, batch):\n",
    "        if self.model_dict['model'] == 'AutoEncoder':\n",
    "            features_c, features = batch\n",
    "            features_c = features_c.cuda().float()\n",
    "            features = features.cuda().float()\n",
    "        elif self.model_dict['model'] == 'MoaDenseNet':\n",
    "            features, targets, ids = batch\n",
    "            features = {k: v.cuda().float() for k, v in features.items()}\n",
    "            targets = targets.cuda().float()\n",
    "        return features, targets\n",
    "        \n",
    "    def train_epoch(self, train_dataloader):\n",
    "        losses = []\n",
    "        self.model.train()\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            features, targets = self.unwrap_batch(batch)\n",
    "            \n",
    "            output = self.model(features)\n",
    "            \n",
    "            loss = self.criterion(output['prediction'], targets)\n",
    "            for p in self.model.parameters():\n",
    "                p.grad = None\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if self.model_dict['scheduler'] == 'OneCycleLR':\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "        avg_loss = float(np.mean(losses))\n",
    "        self.train_metrics.add(avg_loss)\n",
    "        return avg_loss\n",
    "\n",
    "    def validation(self, valid_dataloader, return_preds=False):\n",
    "        losses = []\n",
    "        predictions = []\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch in valid_dataloader:\n",
    "            features, targets = self.unwrap_batch(batch)\n",
    "            \n",
    "            output = self.model(features)\n",
    "            loss = self.criterion(output['prediction'], targets)\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "            if return_preds:\n",
    "                predictions.extend(output['prediction'].detach().cpu().numpy())\n",
    "\n",
    "        avg_loss = float(np.mean(losses))\n",
    "\n",
    "        if return_preds:\n",
    "            return avg_loss, predictions\n",
    "        else:\n",
    "            return avg_loss\n",
    "\n",
    "    def train(self, train_dataloader, valid_dataloader):\n",
    "        for epoch in range(self.model_dict['n_epochs']):\n",
    "            self.epoch = epoch\n",
    "            time0 = time.time()\n",
    "\n",
    "            train_avg_loss = self.train_epoch(train_dataloader)\n",
    "            valid_avg_loss = self.validation(valid_dataloader)\n",
    "\n",
    "            if self.epoch == 0 or valid_avg_loss < self.valid_metrics.min():\n",
    "                # new best weights\n",
    "                self.save(self.best_weights_path)\n",
    "            self.valid_metrics.add(valid_avg_loss)\n",
    "\n",
    "            if self.model_dict['scheduler'] != 'OneCycleLR':\n",
    "                self.scheduler.step(valid_avg_loss)\n",
    "\n",
    "            time1 = time.time()\n",
    "            epoch_time = time1 - time0\n",
    "            if self.model_dict['verbose']:\n",
    "                print('Epoch %d/%d Train Loss: %.5f Valid Loss: %.5f Time: %.2f' % (\n",
    "                    epoch+1, self.model_dict['n_epochs'], train_avg_loss, valid_avg_loss, epoch_time\n",
    "                ))\n",
    "\n",
    "        # restore weights from best epoch\n",
    "        self.load(self.best_weights_path)\n",
    "\n",
    "    def predict(self, test_features):\n",
    "        test_dataset = MoaDataset(test_features)\n",
    "        test_dataloader = DataLoader(test_dataset,\n",
    "                                     batch_size=self.model_dict['batch_size'],\n",
    "                                     num_workers=self.model_dict['num_workers'],\n",
    "                                     pin_memory=True)\n",
    "        predictions = []\n",
    "        self.model.eval()\n",
    "        for features, ids in test_dataloader:\n",
    "            features = {k: v.cuda().float() for k, v in features.items()}\n",
    "            pred = self.model(features)\n",
    "            predictions.extend(pred.detach().cpu().numpy())\n",
    "        return predictions\n",
    "\n",
    "    def reset(self):\n",
    "        self.model.load_state_dict(self.init_states['model'])\n",
    "        self.optimizer.load_state_dict(self.init_states['optimizer'])\n",
    "        self.scheduler.load_state_dict(self.init_states['scheduler'])\n",
    "        self.train_metrics.reset()\n",
    "        self.valid_metrics.reset()\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.eval()\n",
    "        torch.save({\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler': self.scheduler.state_dict(),\n",
    "            'train_metrics': self.train_metrics.data,\n",
    "            'valid_metrics': self.valid_metrics.data,\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        state_dict = torch.load(path)\n",
    "        self.model.load_state_dict(state_dict['model'])\n",
    "        self.optimizer.load_state_dict(state_dict['optimizer'])\n",
    "        self.scheduler.load_state_dict(state_dict['scheduler'])\n",
    "        self.train_metrics.load(state_dict['train_metrics'])\n",
    "        self.valid_metrics.load(state_dict['valid_metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "model_config = dict(\n",
    "    model='AutoEncoder',\n",
    "    latent_layer_size=64,\n",
    "    init_dropout=0.2,\n",
    "    dropout=0.25,\n",
    "    normalization='batch',\n",
    "    activation='prelu',\n",
    "    \n",
    "    n_epochs=150,\n",
    "    optimizer='adam',\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-5,\n",
    "    scheduler='OneCycleLR',\n",
    "    verbose=True,\n",
    "    max_lr=0.05,\n",
    "    pct_start=0.05,\n",
    "    div_factor=1e3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Train each autoencoder\n",
    "# for label in ['gene', 'cell']:\n",
    "        \n",
    "#     # Setup model directory\n",
    "#     model_dir = join(SETTINGS['PROJECTS_DIR'], '%s_autoencoder' % label)\n",
    "#     if not exists(model_dir):\n",
    "#         os.mkdir(model_dir)\n",
    "    \n",
    "#     # Load data\n",
    "#     features = ['cp_time', 'cp_dose'] + [c for c in train_features.columns if label[0] + '-' in c]\n",
    "#     x = train_features[features].values\n",
    "#     test_x = test_features[features].values\n",
    "    \n",
    "#     # Setup dataloaders\n",
    "#     train_dataset = AutoEncoderDataset(x)\n",
    "#     train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=4, shuffle=True)\n",
    "#     test_dataset = AutoEncoderDataset(test_x)\n",
    "#     test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=4)\n",
    "\n",
    "#     # Pass variables through model configuration\n",
    "#     model_config['feature_size'] = x.shape[1]\n",
    "#     model_config['model_dir'] = model_dir\n",
    "#     model_config['fold'] = 0\n",
    "#     model_config['steps_per_epoch'] = len(train_dataloader)\n",
    "\n",
    "#     # Initialize model\n",
    "#     model = NeuralNetModel(model_config)\n",
    "\n",
    "#     # Set custom init weights for fast convergence\n",
    "#     init_bias = np.mean(x, axis=0)\n",
    "#     weights = model.model.dense_3.state_dict()\n",
    "#     weights['dense.bias'] = torch.tensor(init_bias)\n",
    "#     model.model.dense_3.load_state_dict(weights)\n",
    "\n",
    "#     # Train model\n",
    "#     model.train(train_dataloader, test_dataloader)\n",
    "\n",
    "#     # Gather losses\n",
    "#     loss = model.valid_metrics.min()\n",
    "#     print('%.5f' % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# label = 'gene'\n",
    "# model_dir = join(SETTINGS['PROJECTS_DIR'], '%s_autoencoder' % label)\n",
    "\n",
    "# features = ['cp_time', 'cp_dose'] + [c for c in train_features.columns if label[0] + '-' in c]\n",
    "# test_x = test_features[features].values\n",
    "# test_dataset = AutoEncoderDataset(test_x)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=4)\n",
    "\n",
    "# # Pass variables through model configuration\n",
    "# model_config['feature_size'] = test_x.shape[1]\n",
    "# model_config['model_dir'] = model_dir\n",
    "# model_config['fold'] = 0\n",
    "# model_config['steps_per_epoch'] = len(test_dataset)\n",
    "\n",
    "# # Initialize model\n",
    "# model = NeuralNetModel(model_config)\n",
    "# model.load(model.best_weights_path)\n",
    "# loss, preds = model.validation(test_dataloader, return_preds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# crit = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# fig, ax = plt.subplots(5, 5, figsize=(20, 20))\n",
    "# for i in range(25):\n",
    "#     rand_idx = int(np.random.random()*len(test_dataset))\n",
    "#     ground_truth = torch.tensor(test_dataset[rand_idx][0])\n",
    "#     prediction = torch.tensor(preds[rand_idx])\n",
    "#     loss = crit(prediction, ground_truth)\n",
    "#     prediction = nn.Sigmoid()(prediction).numpy()\n",
    "    \n",
    "#     ax[i//5, i%5].scatter(ground_truth, prediction)\n",
    "#     ax[i//5, i%5].set_xlim(0,1)\n",
    "#     ax[i//5, i%5].set_ylim(0,1)\n",
    "#     ax[i//5, i%5].set_title('loss: %.3f' % (loss))\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latent feature vectors for train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sig = nn.Sigmoid()\n",
    "\n",
    "# data = {}\n",
    "# for label in ['gene', 'cell']:\n",
    "#     model_dir = join(SETTINGS['PROJECTS_DIR'], '%s_autoencoder' % label)\n",
    "    \n",
    "#     # Load data\n",
    "#     features = ['cp_time', 'cp_dose'] + [c for c in train_features.columns if label[0] + '-' in c]\n",
    "#     x = train_features[features].values\n",
    "#     test_x = test_features[features].values\n",
    "\n",
    "#     # Setup dataloaders\n",
    "#     train_dataset = AutoEncoderDataset(x)\n",
    "#     train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=4)\n",
    "#     test_dataset = AutoEncoderDataset(test_x)\n",
    "#     test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=4)\n",
    "\n",
    "#     # Pass variables through model configuration\n",
    "#     model_config['feature_size'] = test_x.shape[1]\n",
    "#     model_config['model_dir'] = model_dir\n",
    "#     model_config['fold'] = 0\n",
    "#     model_config['steps_per_epoch'] = len(test_dataset)\n",
    "\n",
    "#     # Initialize model\n",
    "#     model = NeuralNetModel(model_config)\n",
    "#     model.load(model.best_weights_path)\n",
    "#     loss, preds = model.validation(train_dataloader, return_preds=True)\n",
    "#     data[label+'_train'] = sig(torch.tensor(preds)).numpy()\n",
    "    \n",
    "#     loss, preds = model.validation(test_dataloader, return_preds=True)\n",
    "#     data[label+'_test'] = sig(torch.tensor(preds)).numpy()\n",
    "    \n",
    "# train_data = np.hstack([np.expand_dims(train_features['sig_id'].values, 1), data['gene_train'], data['cell_train']])\n",
    "# test_data = np.hstack([np.expand_dims(test_features['sig_id'].values, 1), data['gene_test'], data['cell_test']])\n",
    "# columns = ['sig_id'] + ['f-%d' % i for i in range(train_data.shape[1] - 1)]\n",
    "# train_ae_features = pd.DataFrame(train_data, columns=columns)\n",
    "# test_ae_features = pd.DataFrame(test_data, columns=columns)\n",
    "# train_ae_features.to_csv('train_ae_features.csv', index=False)\n",
    "# test_ae_features.to_csv('test_ae_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/anaconda3/envs/ml_py385/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=True, random_state=123 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "train_ae_features = pd.read_csv('train_ae_features.csv')\n",
    "test_ae_features = pd.read_csv('test_ae_features.csv')\n",
    "y = train_targets.values[:, 1:]\n",
    "skf = MultilabelStratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\n",
    "test_dataset = MoaDataset(test_ae_features)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "model_config = dict(\n",
    "    model='MoaDenseNet',\n",
    "    n_hidden_layer=1,\n",
    "    dropout=0.5,\n",
    "    hidden_dim=1024,    \n",
    "    normalization='batch',\n",
    "    activation='relu',\n",
    "    \n",
    "    n_epochs=25,\n",
    "    optimizer='adam',\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-5,\n",
    "    scheduler='OneCycleLR',\n",
    "    verbose=True,\n",
    "    max_lr=0.05,\n",
    "    pct_start=0.05,\n",
    "    div_factor=1e3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = join(SETTINGS['PROJECTS_DIR'], 'ae_classifier')\n",
    "if not exists(model_dir): os.mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 Train Loss: 0.02186 Valid Loss: 0.02084 Time: 0.94\n",
      "Epoch 2/25 Train Loss: 0.02074 Valid Loss: 0.02091 Time: 0.79\n",
      "Epoch 3/25 Train Loss: 0.02038 Valid Loss: 0.02064 Time: 0.83\n",
      "Epoch 4/25 Train Loss: 0.02010 Valid Loss: 0.02006 Time: 0.89\n",
      "Epoch 5/25 Train Loss: 0.01995 Valid Loss: 0.01947 Time: 0.83\n",
      "Epoch 6/25 Train Loss: 0.01982 Valid Loss: 0.01914 Time: 0.87\n",
      "Epoch 7/25 Train Loss: 0.01966 Valid Loss: 0.01903 Time: 0.88\n",
      "Epoch 8/25 Train Loss: 0.01961 Valid Loss: 0.01896 Time: 0.84\n",
      "Epoch 9/25 Train Loss: 0.01948 Valid Loss: 0.01892 Time: 0.86\n",
      "Epoch 10/25 Train Loss: 0.01941 Valid Loss: 0.01888 Time: 0.83\n",
      "Epoch 11/25 Train Loss: 0.01938 Valid Loss: 0.01885 Time: 0.82\n",
      "Epoch 12/25 Train Loss: 0.01932 Valid Loss: 0.01882 Time: 0.86\n",
      "Epoch 13/25 Train Loss: 0.01928 Valid Loss: 0.01879 Time: 0.82\n",
      "Epoch 14/25 Train Loss: 0.01919 Valid Loss: 0.01876 Time: 0.86\n",
      "Epoch 15/25 Train Loss: 0.01917 Valid Loss: 0.01874 Time: 0.84\n",
      "Epoch 16/25 Train Loss: 0.01911 Valid Loss: 0.01870 Time: 0.80\n",
      "Epoch 17/25 Train Loss: 0.01903 Valid Loss: 0.01868 Time: 0.82\n",
      "Epoch 18/25 Train Loss: 0.01897 Valid Loss: 0.01866 Time: 0.83\n",
      "Epoch 19/25 Train Loss: 0.01895 Valid Loss: 0.01864 Time: 0.82\n",
      "Epoch 20/25 Train Loss: 0.01890 Valid Loss: 0.01861 Time: 0.81\n",
      "Epoch 21/25 Train Loss: 0.01883 Valid Loss: 0.01858 Time: 0.85\n",
      "Epoch 22/25 Train Loss: 0.01881 Valid Loss: 0.01855 Time: 0.89\n",
      "Epoch 23/25 Train Loss: 0.01877 Valid Loss: 0.01852 Time: 0.82\n",
      "Epoch 24/25 Train Loss: 0.01873 Valid Loss: 0.01849 Time: 0.82\n",
      "Epoch 25/25 Train Loss: 0.01871 Valid Loss: 0.01846 Time: 0.87\n",
      "Epoch 1/25 Train Loss: 0.02180 Valid Loss: 0.02085 Time: 0.94\n",
      "Epoch 2/25 Train Loss: 0.02080 Valid Loss: 0.02093 Time: 0.82\n",
      "Epoch 3/25 Train Loss: 0.02032 Valid Loss: 0.02069 Time: 0.83\n",
      "Epoch 4/25 Train Loss: 0.02009 Valid Loss: 0.02007 Time: 0.84\n",
      "Epoch 5/25 Train Loss: 0.01994 Valid Loss: 0.01949 Time: 0.89\n",
      "Epoch 6/25 Train Loss: 0.01982 Valid Loss: 0.01914 Time: 0.84\n",
      "Epoch 7/25 Train Loss: 0.01972 Valid Loss: 0.01901 Time: 0.90\n",
      "Epoch 8/25 Train Loss: 0.01954 Valid Loss: 0.01895 Time: 0.92\n",
      "Epoch 9/25 Train Loss: 0.01952 Valid Loss: 0.01891 Time: 0.89\n",
      "Epoch 10/25 Train Loss: 0.01943 Valid Loss: 0.01887 Time: 0.87\n",
      "Epoch 11/25 Train Loss: 0.01939 Valid Loss: 0.01884 Time: 0.85\n",
      "Epoch 12/25 Train Loss: 0.01931 Valid Loss: 0.01881 Time: 0.95\n",
      "Epoch 13/25 Train Loss: 0.01928 Valid Loss: 0.01878 Time: 0.85\n",
      "Epoch 14/25 Train Loss: 0.01922 Valid Loss: 0.01876 Time: 0.86\n",
      "Epoch 15/25 Train Loss: 0.01915 Valid Loss: 0.01873 Time: 0.97\n",
      "Epoch 16/25 Train Loss: 0.01909 Valid Loss: 0.01871 Time: 0.85\n",
      "Epoch 17/25 Train Loss: 0.01909 Valid Loss: 0.01870 Time: 0.88\n",
      "Epoch 18/25 Train Loss: 0.01905 Valid Loss: 0.01867 Time: 0.85\n",
      "Epoch 19/25 Train Loss: 0.01895 Valid Loss: 0.01864 Time: 0.90\n",
      "Epoch 20/25 Train Loss: 0.01894 Valid Loss: 0.01861 Time: 0.91\n",
      "Epoch 21/25 Train Loss: 0.01890 Valid Loss: 0.01859 Time: 0.87\n",
      "Epoch 22/25 Train Loss: 0.01883 Valid Loss: 0.01856 Time: 0.89\n",
      "Epoch 23/25 Train Loss: 0.01880 Valid Loss: 0.01853 Time: 0.87\n",
      "Epoch 24/25 Train Loss: 0.01876 Valid Loss: 0.01850 Time: 0.92\n",
      "Epoch 25/25 Train Loss: 0.01872 Valid Loss: 0.01847 Time: 0.90\n",
      "Epoch 1/25 Train Loss: 0.02176 Valid Loss: 0.02081 Time: 0.94\n",
      "Epoch 2/25 Train Loss: 0.02078 Valid Loss: 0.02092 Time: 0.95\n",
      "Epoch 3/25 Train Loss: 0.02037 Valid Loss: 0.02069 Time: 1.09\n",
      "Epoch 4/25 Train Loss: 0.02013 Valid Loss: 0.02001 Time: 1.12\n",
      "Epoch 5/25 Train Loss: 0.01992 Valid Loss: 0.01941 Time: 0.85\n",
      "Epoch 6/25 Train Loss: 0.01981 Valid Loss: 0.01908 Time: 0.94\n",
      "Epoch 7/25 Train Loss: 0.01975 Valid Loss: 0.01895 Time: 0.90\n",
      "Epoch 8/25 Train Loss: 0.01965 Valid Loss: 0.01889 Time: 0.95\n",
      "Epoch 9/25 Train Loss: 0.01951 Valid Loss: 0.01884 Time: 0.94\n",
      "Epoch 10/25 Train Loss: 0.01945 Valid Loss: 0.01882 Time: 0.90\n",
      "Epoch 11/25 Train Loss: 0.01937 Valid Loss: 0.01878 Time: 1.04\n",
      "Epoch 12/25 Train Loss: 0.01931 Valid Loss: 0.01875 Time: 0.90\n",
      "Epoch 13/25 Train Loss: 0.01923 Valid Loss: 0.01872 Time: 0.94\n",
      "Epoch 14/25 Train Loss: 0.01922 Valid Loss: 0.01868 Time: 0.91\n",
      "Epoch 15/25 Train Loss: 0.01913 Valid Loss: 0.01867 Time: 0.98\n",
      "Epoch 16/25 Train Loss: 0.01910 Valid Loss: 0.01863 Time: 1.02\n",
      "Epoch 17/25 Train Loss: 0.01906 Valid Loss: 0.01861 Time: 0.95\n",
      "Epoch 18/25 Train Loss: 0.01902 Valid Loss: 0.01859 Time: 0.87\n",
      "Epoch 19/25 Train Loss: 0.01896 Valid Loss: 0.01856 Time: 0.94\n",
      "Epoch 20/25 Train Loss: 0.01890 Valid Loss: 0.01853 Time: 0.96\n",
      "Epoch 21/25 Train Loss: 0.01888 Valid Loss: 0.01849 Time: 0.91\n",
      "Epoch 22/25 Train Loss: 0.01887 Valid Loss: 0.01846 Time: 0.99\n",
      "Epoch 23/25 Train Loss: 0.01876 Valid Loss: 0.01843 Time: 1.04\n",
      "Epoch 24/25 Train Loss: 0.01876 Valid Loss: 0.01841 Time: 0.95\n",
      "Epoch 25/25 Train Loss: 0.01872 Valid Loss: 0.01838 Time: 0.88\n",
      "Epoch 1/25 Train Loss: 0.02192 Valid Loss: 0.02090 Time: 0.99\n",
      "Epoch 2/25 Train Loss: 0.02078 Valid Loss: 0.02099 Time: 0.93\n",
      "Epoch 3/25 Train Loss: 0.02038 Valid Loss: 0.02072 Time: 0.93\n",
      "Epoch 4/25 Train Loss: 0.02010 Valid Loss: 0.02016 Time: 0.86\n",
      "Epoch 5/25 Train Loss: 0.01989 Valid Loss: 0.01963 Time: 0.90\n",
      "Epoch 6/25 Train Loss: 0.01980 Valid Loss: 0.01929 Time: 0.91\n",
      "Epoch 7/25 Train Loss: 0.01970 Valid Loss: 0.01915 Time: 1.01\n",
      "Epoch 8/25 Train Loss: 0.01957 Valid Loss: 0.01909 Time: 0.97\n",
      "Epoch 9/25 Train Loss: 0.01951 Valid Loss: 0.01906 Time: 0.94\n",
      "Epoch 10/25 Train Loss: 0.01941 Valid Loss: 0.01902 Time: 0.85\n",
      "Epoch 11/25 Train Loss: 0.01936 Valid Loss: 0.01898 Time: 0.88\n",
      "Epoch 12/25 Train Loss: 0.01926 Valid Loss: 0.01895 Time: 0.88\n",
      "Epoch 13/25 Train Loss: 0.01920 Valid Loss: 0.01892 Time: 0.87\n",
      "Epoch 14/25 Train Loss: 0.01918 Valid Loss: 0.01889 Time: 0.86\n",
      "Epoch 15/25 Train Loss: 0.01915 Valid Loss: 0.01886 Time: 0.89\n",
      "Epoch 16/25 Train Loss: 0.01910 Valid Loss: 0.01883 Time: 0.88\n",
      "Epoch 17/25 Train Loss: 0.01903 Valid Loss: 0.01881 Time: 0.97\n",
      "Epoch 18/25 Train Loss: 0.01897 Valid Loss: 0.01878 Time: 0.96\n",
      "Epoch 19/25 Train Loss: 0.01892 Valid Loss: 0.01875 Time: 1.05\n",
      "Epoch 20/25 Train Loss: 0.01891 Valid Loss: 0.01873 Time: 1.05\n",
      "Epoch 21/25 Train Loss: 0.01882 Valid Loss: 0.01871 Time: 1.16\n",
      "Epoch 22/25 Train Loss: 0.01877 Valid Loss: 0.01868 Time: 1.10\n",
      "Epoch 23/25 Train Loss: 0.01874 Valid Loss: 0.01865 Time: 1.01\n",
      "Epoch 24/25 Train Loss: 0.01873 Valid Loss: 0.01861 Time: 0.93\n",
      "Epoch 25/25 Train Loss: 0.01870 Valid Loss: 0.01858 Time: 0.98\n",
      "Epoch 1/25 Train Loss: 0.02185 Valid Loss: 0.02084 Time: 0.94\n",
      "Epoch 2/25 Train Loss: 0.02085 Valid Loss: 0.02091 Time: 0.88\n",
      "Epoch 3/25 Train Loss: 0.02037 Valid Loss: 0.02063 Time: 0.94\n",
      "Epoch 4/25 Train Loss: 0.02011 Valid Loss: 0.02004 Time: 0.94\n",
      "Epoch 5/25 Train Loss: 0.01993 Valid Loss: 0.01950 Time: 0.98\n",
      "Epoch 6/25 Train Loss: 0.01982 Valid Loss: 0.01919 Time: 0.95\n",
      "Epoch 7/25 Train Loss: 0.01969 Valid Loss: 0.01906 Time: 0.94\n",
      "Epoch 8/25 Train Loss: 0.01959 Valid Loss: 0.01900 Time: 0.94\n",
      "Epoch 9/25 Train Loss: 0.01952 Valid Loss: 0.01895 Time: 0.96\n",
      "Epoch 10/25 Train Loss: 0.01941 Valid Loss: 0.01890 Time: 0.95\n",
      "Epoch 11/25 Train Loss: 0.01936 Valid Loss: 0.01887 Time: 1.02\n",
      "Epoch 12/25 Train Loss: 0.01932 Valid Loss: 0.01883 Time: 1.06\n",
      "Epoch 13/25 Train Loss: 0.01927 Valid Loss: 0.01882 Time: 1.16\n",
      "Epoch 14/25 Train Loss: 0.01922 Valid Loss: 0.01880 Time: 1.18\n",
      "Epoch 15/25 Train Loss: 0.01916 Valid Loss: 0.01876 Time: 0.98\n",
      "Epoch 16/25 Train Loss: 0.01909 Valid Loss: 0.01873 Time: 0.92\n",
      "Epoch 17/25 Train Loss: 0.01906 Valid Loss: 0.01870 Time: 1.25\n",
      "Epoch 18/25 Train Loss: 0.01902 Valid Loss: 0.01867 Time: 1.13\n",
      "Epoch 19/25 Train Loss: 0.01897 Valid Loss: 0.01864 Time: 0.99\n",
      "Epoch 20/25 Train Loss: 0.01891 Valid Loss: 0.01862 Time: 1.07\n",
      "Epoch 21/25 Train Loss: 0.01888 Valid Loss: 0.01860 Time: 0.91\n",
      "Epoch 22/25 Train Loss: 0.01881 Valid Loss: 0.01856 Time: 0.93\n",
      "Epoch 23/25 Train Loss: 0.01876 Valid Loss: 0.01853 Time: 1.21\n",
      "Epoch 24/25 Train Loss: 0.01872 Valid Loss: 0.01850 Time: 1.27\n",
      "Epoch 25/25 Train Loss: 0.01869 Valid Loss: 0.01849 Time: 0.95\n"
     ]
    }
   ],
   "source": [
    "for fold_i, (train_idx, valid_idx) in enumerate(skf.split(train_ae_features, y)):\n",
    "    train_x, train_y = train_ae_features.iloc[train_idx], train_targets.iloc[train_idx]\n",
    "    valid_x, valid_y = train_ae_features.iloc[valid_idx], train_targets.iloc[valid_idx]\n",
    "    \n",
    "    # Setup dataloaders\n",
    "    train_dataset = MoaDataset(train_x, train_y)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=4, shuffle=True)\n",
    "    valid_dataset = MoaDataset(valid_x, valid_y)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, pin_memory=True, num_workers=4)\n",
    "\n",
    "    # Pass variables through model configuration\n",
    "    model_config['input_dim'] = len(train_x.columns) - 1\n",
    "    model_config['output_dim'] = len(train_y.columns) - 1\n",
    "    model_config['model_dir'] = model_dir\n",
    "    model_config['fold'] = fold_i\n",
    "    model_config['steps_per_epoch'] = len(train_dataset)\n",
    "    \n",
    "    # train neural network\n",
    "    model = NeuralNetModel(model_config)\n",
    "    model.model.layers = initialize_weights(model.model.layers)\n",
    "    model.train(train_dataloader, valid_dataloader)\n",
    "    \n",
    "    \n",
    "    # logistic regression\n",
    "    \n",
    "    # xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml 3.8.5)",
   "language": "python",
   "name": "ml_py385"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
